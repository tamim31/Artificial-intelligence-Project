{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Factors Associated with Teenage Alcohol Abuse\n",
    "#### --- Authors: Eon Slemp and Lhamu Tsering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction- Overview\n",
    "\n",
    "The New York City Department of Education wants a model to help identify students at risk of alcohol abuse. Identifying the factors that influence youth alcohol abuse, can help\n",
    "* Prioritize resources \n",
    "* Develop new interventions\n",
    "* Reduce likelihood of dropout\n",
    "* Improve life outcomes for NYC students\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Business Problem\n",
    "\n",
    "Alcohol abuse among teenage students is a national problem, according to the [CDC](https://www.cdc.gov/alcohol/fact-sheets/underage-drinking.htm). The teenage brain is not biologically fully developed. One of the dangerous effects of youth alcohol abuse is the risk of inhibiting proper brain development. According to Dr. Marisa M. Silveri, director of [McLean Hospital’s Neurodevelopmental Laboratory on Addictions and Mental Health](https://www.mcleanhospital.org/news/essence-adolescence-examining-addiction-teenage-brain), \"age of first alcohol use is a strong predictor of an alcohol abuse problem later in life. If you’re 13 when you start drinking, you have a 47% chance of having a problem as an adult. If you wait until 21, you still have a 9% chance that you’ll have a problem later on, but there’s a clear pattern—the longer you can delay onset of use, the more you can protect your brain.\" Other negative effects of adolescent alcohol abuse is memory difficulties. \n",
    "\n",
    "We say that the children and youth are the pillars of the future. So, to be able to protect and prevent young students from becoming heavy alcohol drinkers becomes very important. One of the way to do this, is to first identify what factors or condition in a student or teenager's life makes them take decisions that lead them to become heavy drinkers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Understanding\n",
    "\n",
    "The data for this data science project in researching factors influencing teenage alcoholism was sourced from the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/student%2Bperformance#). It was donated to the site by Prof. Paulo Cortez of University Minho. His original work on the dataset, \"USING DATA MINING TO PREDICT SECONDARY SCHOOL STUDENT PERFORMANCE, can be found [here](http://www3.dsi.uminho.pt/pcortez/student.pdf)\n",
    "\n",
    "The data set consists of information on various attributes for each student, taking Portuguese language classes who come from ether of the two higher secondary schools, The Gabriel Pereira School and  the Mousinho da Silveira School. There is information on 649 students on 33 attributes. A list of all the features with description can be found on [Readme](https://github.com/Yeshi341/Student_Alcohol_Consumption/blob/master/Readme.md) section of the Github page to this project. The features have also been described sequentially as [EDA]('EDA.ipynb') was performed on each variable in the EDA notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading dataset\n",
    "df = pd.read_csv('student-por.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation- data cleaning and transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows and Columns : (649, 33) \n",
      "\n",
      "INFO:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 649 entries, 0 to 648\n",
      "Data columns (total 33 columns):\n",
      "school        649 non-null object\n",
      "sex           649 non-null object\n",
      "age           649 non-null int64\n",
      "address       649 non-null object\n",
      "famsize       649 non-null object\n",
      "Pstatus       649 non-null object\n",
      "Medu          649 non-null int64\n",
      "Fedu          649 non-null int64\n",
      "Mjob          649 non-null object\n",
      "Fjob          649 non-null object\n",
      "reason        649 non-null object\n",
      "guardian      649 non-null object\n",
      "traveltime    649 non-null int64\n",
      "studytime     649 non-null int64\n",
      "failures      649 non-null int64\n",
      "schoolsup     649 non-null object\n",
      "famsup        649 non-null object\n",
      "paid          649 non-null object\n",
      "activities    649 non-null object\n",
      "nursery       649 non-null object\n",
      "higher        649 non-null object\n",
      "internet      649 non-null object\n",
      "romantic      649 non-null object\n",
      "famrel        649 non-null int64\n",
      "freetime      649 non-null int64\n",
      "goout         649 non-null int64\n",
      "Dalc          649 non-null int64\n",
      "Walc          649 non-null int64\n",
      "health        649 non-null int64\n",
      "absences      649 non-null int64\n",
      "G1            649 non-null int64\n",
      "G2            649 non-null int64\n",
      "G3            649 non-null int64\n",
      "dtypes: int64(16), object(17)\n",
      "memory usage: 167.4+ KB\n",
      "None \n",
      "\n",
      "Unique Values : \n",
      " school         2\n",
      "sex            2\n",
      "age            8\n",
      "address        2\n",
      "famsize        2\n",
      "Pstatus        2\n",
      "Medu           5\n",
      "Fedu           5\n",
      "Mjob           5\n",
      "Fjob           5\n",
      "reason         4\n",
      "guardian       3\n",
      "traveltime     4\n",
      "studytime      4\n",
      "failures       4\n",
      "schoolsup      2\n",
      "famsup         2\n",
      "paid           2\n",
      "activities     2\n",
      "nursery        2\n",
      "higher         2\n",
      "internet       2\n",
      "romantic       2\n",
      "famrel         5\n",
      "freetime       5\n",
      "goout          5\n",
      "Dalc           5\n",
      "Walc           5\n",
      "health         5\n",
      "absences      24\n",
      "G1            17\n",
      "G2            16\n",
      "G3            17\n",
      "dtype: int64 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Rows tell you the number of obs and columns tell you the number of features\n",
    "print('Rows and Columns :', df.shape,'\\n' ) \n",
    "\n",
    "#Running info on the dataset to check on the any visible missing values and datatypes\n",
    "print('INFO:')\n",
    "print(df.info(), '\\n')\n",
    "\n",
    "# Checking for the number of unique values in each column in dataset\n",
    "print('Unique Values :','\\n', df.nunique(), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting all binary non-numeric variables to numeric variables to allow for easy data processing\n",
    "\n",
    "df.loc[df['school']=='GP', 'school'] = 1\n",
    "df.loc[df['school']=='MS', 'school'] = 0\n",
    "\n",
    "df.loc[df['sex']=='F', 'sex'] = 1\n",
    "df.loc[df['sex']=='M', 'sex'] = 0\n",
    "\n",
    "df.loc[df['address']=='R', 'address'] = 1\n",
    "df.loc[df['address']=='U', 'address'] = 0\n",
    "\n",
    "df.loc[df['famsize']=='GT3', 'famsize'] = 1\n",
    "df.loc[df['famsize']=='LE3', 'famsize'] = 0\n",
    "\n",
    "df.loc[df['Pstatus']=='T', 'Pstatus'] = 1\n",
    "df.loc[df['Pstatus']=='A', 'Pstatus'] = 0\n",
    "\n",
    "df.loc[df['schoolsup']=='yes', 'schoolsup'] = 1\n",
    "df.loc[df['schoolsup']=='no', 'schoolsup'] = 0\n",
    "\n",
    "df.loc[df['famsup']=='yes', 'famsup'] = 1\n",
    "df.loc[df['famsup']=='no', 'famsup'] = 0\n",
    "\n",
    "df.loc[df['paid']=='yes', 'paid'] = 1\n",
    "df.loc[df['paid']=='no', 'paid'] = 0\n",
    "\n",
    "df.loc[df['activities']=='yes', 'activities'] = 1\n",
    "df.loc[df['activities']=='no', 'activities'] = 0\n",
    "\n",
    "df.loc[df['nursery']=='yes', 'nursery'] = 1\n",
    "df.loc[df['nursery']=='no', 'nursery'] = 0\n",
    "\n",
    "df.loc[df['higher']=='yes', 'higher'] = 1\n",
    "df.loc[df['higher']=='no', 'higher'] = 0\n",
    "\n",
    "df.loc[df['internet']=='yes', 'internet'] = 1\n",
    "df.loc[df['internet']=='no', 'internet'] = 0\n",
    "\n",
    "df.loc[df['romantic']=='yes', 'romantic'] = 1\n",
    "df.loc[df['romantic']=='no', 'romantic'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting all nominal non-numeric variables to numeric variables to allow for easy data processing\n",
    "\n",
    "conditions=[df['Mjob'] == 'at_home',\n",
    "          df['Mjob'] == 'services',\n",
    "          df['Mjob'] == 'teacher',\n",
    "          df['Mjob'] == 'health',\n",
    "           df['Mjob']== 'other']\n",
    "choices = [1,2,3,4,5]\n",
    "df['Mjob'] = np.select(conditions, choices)\n",
    "\n",
    "conditions=[df['Fjob'] == 'at_home',\n",
    "          df['Fjob'] == 'services',\n",
    "          df['Fjob'] == 'teacher',\n",
    "          df['Fjob'] == 'health',\n",
    "           df['Fjob']== 'other']\n",
    "choices = [1,2,3,4,5]\n",
    "df['Fjob'] = np.select(conditions, choices)\n",
    "\n",
    "conditions=[df['reason'] == 'home',\n",
    "          df['reason'] == 'reputation',\n",
    "          df['reason'] == 'course',\n",
    "           df['reason']== 'other']\n",
    "choices = [1,2,3,4]\n",
    "df['reason'] = np.select(conditions, choices)\n",
    "\n",
    "conditions=[df['guardian'] == 'father',\n",
    "          df['guardian'] == 'mother',\n",
    "          df['guardian'] == 'other']\n",
    "choices = [1,2,3]\n",
    "df['guardian'] = np.select(conditions, choices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "school        int64\n",
       "sex           int64\n",
       "age           int64\n",
       "address       int64\n",
       "famsize       int64\n",
       "Pstatus       int64\n",
       "Medu          int64\n",
       "Fedu          int64\n",
       "Mjob          int64\n",
       "Fjob          int64\n",
       "reason        int64\n",
       "guardian      int64\n",
       "traveltime    int64\n",
       "studytime     int64\n",
       "failures      int64\n",
       "schoolsup     int64\n",
       "famsup        int64\n",
       "paid          int64\n",
       "activities    int64\n",
       "nursery       int64\n",
       "higher        int64\n",
       "internet      int64\n",
       "romantic      int64\n",
       "famrel        int64\n",
       "freetime      int64\n",
       "goout         int64\n",
       "Dalc          int64\n",
       "Walc          int64\n",
       "health        int64\n",
       "absences      int64\n",
       "G1            int64\n",
       "G2            int64\n",
       "G3            int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2     241\n",
       "3     116\n",
       "4      99\n",
       "5      73\n",
       "6      50\n",
       "7      32\n",
       "8      17\n",
       "10     15\n",
       "9       6\n",
       "Name: week_alc, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['week_alc']= df['Dalc']+df['Walc']\n",
    "\n",
    "df['week_alc'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    529\n",
      "1    120\n",
      "Name: alc, dtype: int64\n",
      "0    0.8151\n",
      "1    0.1849\n",
      "Name: alc, dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1, 'Distribution of Heavy Drinkers VS Light drinkers')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df['alc'] = [0 if x <= 5 else 1 for x in df['week_alc']]\n",
    "        \n",
    "print(df['alc'].value_counts())\n",
    "print(df['alc'].value_counts(normalize = True))\n",
    "\n",
    "sns.catplot(x=\"alc\", kind=\"count\", palette=\"ch:.25\", data=df)\n",
    "plt.title('Distribution of Heavy Drinkers VS Light drinkers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "school        False\n",
       "sex           False\n",
       "age           False\n",
       "address       False\n",
       "famsize       False\n",
       "Pstatus       False\n",
       "Medu          False\n",
       "Fedu          False\n",
       "Mjob          False\n",
       "Fjob          False\n",
       "reason        False\n",
       "guardian      False\n",
       "traveltime    False\n",
       "studytime     False\n",
       "failures      False\n",
       "schoolsup     False\n",
       "famsup        False\n",
       "paid          False\n",
       "activities    False\n",
       "nursery       False\n",
       "higher        False\n",
       "internet      False\n",
       "romantic      False\n",
       "famrel        False\n",
       "freetime      False\n",
       "goout         False\n",
       "Dalc          False\n",
       "Walc          False\n",
       "health        False\n",
       "absences      False\n",
       "G1            False\n",
       "G2            False\n",
       "G3            False\n",
       "week_alc      False\n",
       "alc           False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking for Nan values\n",
    "df.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "279    22\n",
       "Name: age, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['age']== 22]['age']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], Name: age, dtype: int64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop(labels = 279, inplace=True) #dropping age outlier\n",
    "df[df['age']== 22]['age']# checking to see if the drop was successful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_df = df.drop(columns = ['Dalc', 'Walc', 'week_alc', 'alc'], axis = 1) # grabs everything else but 'default column'\n",
    "y = df['alc']\n",
    "orig_feats = list(X_df.columns)\n",
    "len(orig_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis === Check here for [EDA]('EDA.ipynb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature quantifying family stability "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['stability'] = df['famrel'] * df['famsize']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Descriptive Statistics :\", '\\n', df['stability'].describe())\n",
    "print(\"Correlation with target var 'alc consumption' :\", '\\n', df['stability'].corr(df['alc']))\n",
    "print(\"percentage of data distrib :\", '\\n', df.stability.value_counts(normalize=True))\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize= (10,6))\n",
    "sns.despine(right=True)\n",
    "sns.countplot(x=\"stability\", data=df,  ax = ax[0])\n",
    "sns.barplot(x=\"stability\", y = 'alc', data=df,  ax = ax[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "feature quantifying academic support network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['academic_support'] = df.famsup + df.schoolsup  + df.higher "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Descriptive Statistics :\", '\\n', df['academic_support'].describe())\n",
    "print(\"Correlation with target var 'alc consumption' :\", '\\n', df['academic_support'].corr(df['alc']))\n",
    "print(\"percentage of data distrib :\", '\\n', df.academic_support.value_counts(normalize=True))\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize= (10,6))\n",
    "sns.despine(right=True)\n",
    "sns.countplot(x=\"academic_support\", data=df,  ax = ax[0])\n",
    "sns.barplot(x=\"academic_support\",  y ='alc', data=df,  ax = ax[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['idle'] = df.goout*df.freetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Descriptive Statistics :\", '\\n', df['idle'].describe())\n",
    "print(\"Correlation with target variable 'alc consumption' :\", '\\n', df['idle'].corr(df['alc']))\n",
    "print(\"percentage of data distrib :\", '\\n', df.idle.value_counts(normalize=True))\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize= (10,6))\n",
    "sns.despine(right=True)\n",
    "sns.countplot(x=\"idle\", data=df,  ax = ax[0])\n",
    "ax[0].set_title('abc')\n",
    "sns.barplot(x=\"idle\", y = 'alc', data=df,  ax = ax[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average Grade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['grade_avg'] = round((df.G1 + df.G2 + df.G3)/3, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Descriptive Statistics :\", '\\n', df['grade_avg'].describe())\n",
    "print(\"Correlation with target alc consumption :\", '\\n', df['grade_avg'].corr(df['alc']))\n",
    "\n",
    "sns.despine(right=True)\n",
    "sns.distplot(a=df[\"grade_avg\"],bins=10, kde=True, hist=True)\n",
    "\n",
    "# plt.figure(figsize=(5,8))\n",
    "# df.boxplot(column='grade_avg', by = 'alc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heavy = df.loc[df['alc'] == 1]\n",
    "light = df.loc[df['alc'] == 0]\n",
    "grades_heavy = df.grade_avg\n",
    "grades_light = df.grade_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist([grades_heavy,grades_light], bins = 30, stacked = True)\n",
    "plt.xlabel('grade point average')\n",
    "plt.ylabel('frequency')\n",
    "plt.title('heavy drinkers & light drinkers GPA distributions')\n",
    "plt.axvline(x = 10.5, color = 'k')\n",
    "plt.axvline(x = 12.33, color = 'k')\n",
    "plt.savefig('images/gpahist.png')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['delinquency'] = (df['failures']) * df['absences']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Descriptive Statistics :\", '\\n', df['delinquency'].describe())\n",
    "print(\"Correlation with target variable 'alc consumption' :\", '\\n', df['delinquency'].corr(df['alc']))\n",
    "print(\"percentage of data distrib :\", '\\n', df.delinquency.value_counts(normalize=True))\n",
    "\n",
    "sns.despine(right=True)\n",
    "sns.distplot(a=df[\"delinquency\"],bins=10, kde = False ,hist=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.shape)\n",
    "\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy1 = pd.get_dummies(df['Medu'],drop_first= True, prefix='Medu')\n",
    "dummy2 = pd.get_dummies(df['Fedu'],drop_first= True, prefix='Fedu')\n",
    "dummy3 = pd.get_dummies(df['Mjob'],drop_first= True, prefix='Mjob')\n",
    "dummy4 = pd.get_dummies(df['Fjob'],drop_first= True, prefix='Fjob')\n",
    "dummy5 = pd.get_dummies(df['reason'],drop_first= True, prefix='reason')\n",
    "dummy6 = pd.get_dummies(df['guardian'],drop_first= True, prefix='guardian')\n",
    "dummy7 = pd.get_dummies(df['traveltime'],drop_first= True, prefix='traveltime')\n",
    "dummy8 = pd.get_dummies(df['studytime'],drop_first= True, prefix='studytime')\n",
    "dummy9 = pd.get_dummies(df['failures'],drop_first= True, prefix = 'failures')\n",
    "dummy10 = pd.get_dummies(df['famrel'],drop_first= True, prefix='famrel')\n",
    "dummy11 = pd.get_dummies(df['freetime'],drop_first= True, prefix='freetime')\n",
    "dummy12 = pd.get_dummies(df['goout'],drop_first= True, prefix=\"goout\")\n",
    "dummy13 = pd.get_dummies(df['health'],drop_first= True, prefix='health')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_feats = ['grade_avg', 'academic_support', 'stability', 'idle', 'delinquency']\n",
    "dummy_feats = ['Medu', 'Fedu','Mjob', 'Fjob', 'reason', 'guardian', 'traveltime', 'studytime',\n",
    "       'failures','famrel', 'freetime', 'goout','health']\n",
    "\n",
    "df.drop(columns = ['week_alc', 'Dalc', 'Walc', 'alc'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = [df, dummy1, dummy2, dummy3, dummy4, dummy5, dummy6, dummy7, \n",
    "           dummy8, dummy9, dummy10, dummy11, dummy12, dummy13]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in df_list:\n",
    "    print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.concat(df_list, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#performing train-test split on main dataframe\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=150, test_size=0.2)\n",
    "\n",
    "#checking the shape of the training set and test set\n",
    "print(\"Training set - Features: \", X_train.shape, \"Target: \", y_train.shape,)\n",
    "print(\"Test set - Features: \", X_test.shape, \"Target: \",y_test.shape,)\n",
    "print(y_train.value_counts(normalize = True))\n",
    "print(y_test.value_counts(normalize = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Class Imbalance - with Oversampling minority class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upsampling Minority"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate our training data back together\n",
    "training  = pd.concat([X_train, y_train], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate minority and majority classes\n",
    "light_drinker = training[training.alc==0]\n",
    "heavy_drinker = training[training.alc==1]\n",
    "\n",
    "# Get a class count to understand the class imbalance.\n",
    "print('light drinker count: '+ str(len(light_drinker)))\n",
    "print('heavy drinker count: '+ str(len(heavy_drinker)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upsample minority\n",
    "heavy_drinker_upsampled = resample(heavy_drinker,\n",
    "                          replace=True, # sample with replacement\n",
    "                          n_samples=len(light_drinker), # match number in majority class\n",
    "                          random_state=23) # reproducible results\n",
    "heavy_drinker_upsampled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine majority and upsampled minority\n",
    "upsampled = pd.concat([light_drinker, heavy_drinker_upsampled])\n",
    "\n",
    "# check new class counts\n",
    "upsampled.alc.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trying logistic regression again with the balanced dataset\n",
    "y_train_up = upsampled.alc\n",
    "X_train_up = upsampled.drop(columns = 'alc', axis=1)\n",
    "X_train_up.drop(columns = dummy_feats, axis=1, inplace=True)\n",
    "X_train_up.shape\n",
    "\n",
    "# print(y_train_u.value_counts(normalize = True))\n",
    "# print(X_train_u.value_counts(normalize = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(orig_feats))\n",
    "print(len(new_feats))\n",
    "print(len(dummy_feats))\n",
    "print(X_train_up.shape)\n",
    "print(y_train_up.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select Kbest 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting parameters to choose the best 10 variables based on the significance of the variable F stat\n",
    "selector = SelectKBest(f_classif, k=10) #setting parameters to choose the best 10 variables based on the significance of the variable F stat\n",
    "\n",
    "#Fitting selector object to training set\n",
    "selector.fit(X_train_up, y_train_up)\n",
    "\n",
    "selected_columns = X_train_up.columns[selector.get_support()]\n",
    "removed_columns = X_train_up.columns[~selector.get_support()]\n",
    "\n",
    "X_train_kb10 = X_train_up[selected_columns]\n",
    "X_test_kb10 = X_test[selected_columns]\n",
    "print(X_train_kb10.shape, X_test_kb10.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select Kbest 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting parameters to choose the best 50 variables based on the significance of the variable F stat\n",
    "selector = SelectKBest(f_classif, k=15) #setting parameters to choose the best 50 variables based on the significance of the variable F stat\n",
    "\n",
    "#Fitting selector object to training set\n",
    "selector.fit(X_train_up, y_train_up)\n",
    "\n",
    "selected_columns = X_train_up.columns[selector.get_support()]\n",
    "removed_columns = X_train_up.columns[~selector.get_support()]\n",
    "\n",
    "X_train_kb15 = X_train_up[selected_columns]\n",
    "X_test_kb15 = X_test[selected_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select Kbest 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting parameters to choose the best 50 variables based on the significance of the variable F stat\n",
    "selector = SelectKBest(f_classif, k=20) #setting parameters to choose the best 50 variables based on the significance of the variable F stat\n",
    "\n",
    "#Fitting selector object to training set\n",
    "selector.fit(X_train_up, y_train_up)\n",
    "\n",
    "selected_columns = X_train_up.columns[selector.get_support()]\n",
    "removed_columns = X_train_up.columns[~selector.get_support()]\n",
    "\n",
    "X_train_kb20 = X_train_up[selected_columns]\n",
    "X_test_kb20 = X_test[selected_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select Kbest 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting parameters to choose the best 50 variables based on the significance of the variable F stat\n",
    "selector = SelectKBest(f_classif, k=25) #setting parameters to choose the best 50 variables based on the significance of the variable F stat\n",
    "\n",
    "#Fitting selector object to training set\n",
    "selector.fit(X_train_up, y_train_up)\n",
    "\n",
    "selected_columns = X_train_up.columns[selector.get_support()]\n",
    "removed_columns = X_train_up.columns[~selector.get_support()]\n",
    "\n",
    "X_train_kb25 = X_train_up[selected_columns]\n",
    "X_test_kb25 = X_test[selected_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select Kbest 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting parameters to choose the best 50 variables based on the significance of the variable F stat\n",
    "selector = SelectKBest(f_classif, k=30) #setting parameters to choose the best 50 variables based on the significance of the variable F stat\n",
    "\n",
    "#Fitting selector object to training set\n",
    "selector.fit(X_train_up, y_train_up)\n",
    "\n",
    "selected_columns = X_train_up.columns[selector.get_support()]\n",
    "removed_columns = X_train_up.columns[~selector.get_support()]\n",
    "\n",
    "X_train_kb30 = X_train_up[selected_columns]\n",
    "X_test_kb30 = X_test[selected_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select Kbest 35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting parameters to choose the best 50 variables based on the significance of the variable F stat\n",
    "selector = SelectKBest(f_classif, k=35) #setting parameters to choose the best 50 variables based on the significance of the variable F stat\n",
    "\n",
    "#Fitting selector object to training set\n",
    "selector.fit(X_train_up, y_train_up)\n",
    "\n",
    "selected_columns = X_train_up.columns[selector.get_support()]\n",
    "removed_columns = X_train_up.columns[~selector.get_support()]\n",
    "\n",
    "X_train_kb35 = X_train_up[selected_columns]\n",
    "X_test_kb35 = X_test[selected_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our target variable in this project, tells whether a student is a heavy alcohol drinker or not. Our main concern here becomes that we do not want to predict that a student is not a heavy drinker when they actually are. Thus, we are interested in minimizing chances of any False Negatives. Correctly, identifying student has a problem allows us to appropriately allocate help or resources to ameliorate conditions for that student/s to minimize any drinking problem. \n",
    "\n",
    "Thus, our focus will be on the recall score or sensitivity score that tells us the proportion of actual positives identified correctly, given by (TP/(TP+FN). The higher this score, the better. \n",
    "\n",
    "We also looked at the Accuracy score and the F1 scores as extra metrics to compare model performance on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. BASELINE Model\n",
    "**Running a model without any class imbalance resolution on the original features with no transformations or scaling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr1 = LogisticRegression(solver='liblinear', random_state=150)\n",
    "\n",
    "lr1.fit(X_train[orig_feats], y_train)\n",
    "\n",
    "y_pred_test = lr1.predict(X_test[orig_feats])\n",
    "y_pred_train = lr1.predict(X_train[orig_feats])\n",
    "\n",
    "# checking accuracy\n",
    "print('Test Accuracy score: ', metrics.accuracy_score(y_test, y_pred_test))\n",
    "print('Train Accuracy score: ', metrics.accuracy_score(y_train, y_pred_train))\n",
    "\n",
    "# checking accuracy\n",
    "print('Test F1 score: ', metrics.f1_score(y_test, y_pred_test))\n",
    "print('Train F1 score: ', metrics.f1_score(y_train, y_pred_train))\n",
    "\n",
    "# checking accuracy\n",
    "print('Test Recall score: ', metrics.recall_score(y_test, y_pred_test))\n",
    "print('Train Recall score: ', metrics.recall_score(y_train, y_pred_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "\n",
    "results['1.lr_baseline'] = (round(metrics.accuracy_score(y_test, y_pred_test),4), \n",
    "                            round(metrics.f1_score(y_test, y_pred_test),4), \n",
    "                          round(metrics.recall_score(y_test, y_pred_test),4))\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing the dummied features from the train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train1 = X_train.drop(columns = dummy_feats, axis = 1)\n",
    "X_test1 = X_test.drop(columns = dummy_feats, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler1 = MinMaxScaler()  \n",
    "scaler1.fit(X_train1)\n",
    "# Scaling Imbalanced data\n",
    "X_train_scaleI = scaler1.transform(X_train1)  \n",
    "X_test_scale = scaler1.transform(X_test1)\n",
    "\n",
    "#Scaling upsampled data\n",
    "scaler2 = MinMaxScaler()  \n",
    "scaler2.fit(X_train_up)\n",
    "\n",
    "X_train_scaleu = scaler2.transform(X_train_up)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape) # All features train set\n",
    "print(X_test.shape) # All features test set\n",
    "\n",
    "print(X_train1.shape) #Removed dummied features\n",
    "print(X_test1.shape) # Removed dummied features\n",
    "\n",
    "print(X_train_up.shape) #Upsampled train set and no dummied vars\n",
    "print(y_train_up.shape)\n",
    "\n",
    "print(X_train_scaleI.shape) # Scaled Imbalanced X_train1 set\n",
    "\n",
    "print(X_train_scaleu.shape) # Scaled upsampled X_train_up set\n",
    "print(X_test_scale.shape) # Scaled X_test1 set\n",
    "\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Logistic Regression with X_train_up (Upsampled Unscaled Train set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upsampled_lr = LogisticRegression(solver='liblinear')\n",
    "\n",
    "\n",
    "upsampled_lr.fit(X_train_up, y_train_up)\n",
    "\n",
    "\n",
    "upsampled_pred_test = upsampled_lr.predict(X_test1)\n",
    "upsampled_pred_train = upsampled_lr.predict(X_train_up)\n",
    "\n",
    "\n",
    "# checking accuracy\n",
    "print('Test Accuracy score: ', metrics.accuracy_score(y_test, upsampled_pred_test))\n",
    "print('Train Accuracy score: ', metrics.accuracy_score(y_train_up, upsampled_pred_train))\n",
    "\n",
    "# checking accuracy\n",
    "print('Test F1 score: ', metrics.f1_score(y_test, upsampled_pred_test))\n",
    "print('Train F1 score: ', metrics.f1_score(y_train_up, upsampled_pred_train))\n",
    "\n",
    "# checking accuracy\n",
    "print('Test Recall score: ', metrics.recall_score(y_test, upsampled_pred_test))\n",
    "print('Train Recall score: ', metrics.recall_score(y_train_up, upsampled_pred_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results['2. lr_upsampled'] = (round(metrics.accuracy_score(y_test, upsampled_pred_test),4), \n",
    "                           round(metrics.f1_score(y_test, upsampled_pred_test),4),\n",
    "                          round(metrics.recall_score(y_test, upsampled_pred_test)))\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Logistic Regression with X_train_up (Upsampled and Scaled Train set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upscaled_lr = LogisticRegression(solver='liblinear')\n",
    "\n",
    "\n",
    "upscaled_lr.fit(X_train_scaleu, y_train_up)\n",
    "\n",
    "\n",
    "upscaled_pred_test = upsampled_lr.predict(X_test_scale)\n",
    "upscaled_pred_train = upsampled_lr.predict(X_train_scaleu)\n",
    "\n",
    "\n",
    "# checking accuracy\n",
    "print('Test Accuracy score: ', metrics.accuracy_score(y_test, upscaled_pred_test))\n",
    "print('Train Accuracy score: ', metrics.accuracy_score(y_train_up, upscaled_pred_train))\n",
    "\n",
    "# checking accuracy\n",
    "print('Test F1 score: ', metrics.f1_score(y_test, upscaled_pred_test))\n",
    "print('Train F1 score: ', metrics.f1_score(y_train_up, upscaled_pred_train))\n",
    "\n",
    "# checking accuracy\n",
    "print('Test Recall score: ', metrics.recall_score(y_test, upscaled_pred_test))\n",
    "print('Train Recall score: ', metrics.recall_score(y_train_up, upscaled_pred_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results['3.  lr_upscaled'] = (round(metrics.accuracy_score(y_test, upscaled_pred_test),4), \n",
    "                           round(metrics.f1_score(y_test, upscaled_pred_test),4),\n",
    "                          round(metrics.recall_score(y_train_up, upscaled_pred_train),4))\n",
    "results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Logistic Regression with K best 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kb10_lr = LogisticRegression(solver='liblinear')\n",
    "\n",
    "\n",
    "kb10_lr.fit(X_train_kb10, y_train_up)\n",
    "\n",
    "\n",
    "kb10_pred_test = kb10_lr.predict(X_test_kb10)\n",
    "kb10_pred_train = kb10_lr.predict(X_train_kb10)\n",
    "\n",
    "\n",
    "# checking accuracy\n",
    "print('Test Accuracy score: ', metrics.accuracy_score(y_test, kb10_pred_test))\n",
    "print('Train Accuracy score: ', metrics.accuracy_score(y_train_up, kb10_pred_train))\n",
    "\n",
    "# checking accuracy\n",
    "print('Test F1 score: ', metrics.f1_score(y_test, kb10_pred_test))\n",
    "print('Train F1 score: ', metrics.f1_score(y_train_up, kb10_pred_train))\n",
    "\n",
    "# checking accuracy\n",
    "print('Test Recall score: ', metrics.recall_score(y_test, kb10_pred_test))\n",
    "print('Train Recall score: ', metrics.recall_score(y_train_up, kb10_pred_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results['4. lr_kb10'] = (round(metrics.accuracy_score(y_test, kb10_pred_test),4), \n",
    "                           round(metrics.f1_score(y_test, kb10_pred_test),4),\n",
    "                     round(metrics.recall_score(y_test, kb10_pred_test),4))\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Logistic Regression with K best 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kb15_lr = LogisticRegression(solver='liblinear')\n",
    "\n",
    "kb15_lr.fit(X_train_kb15, y_train_up)\n",
    "\n",
    "\n",
    "kb15_pred_test = kb15_lr.predict(X_test_kb15)\n",
    "kb15_pred_train = kb15_lr.predict(X_train_kb15)\n",
    "\n",
    "\n",
    "# checking accuracy\n",
    "print('Test Accuracy score: ', metrics.accuracy_score(y_test, kb15_pred_test))\n",
    "print('Train Accuracy score: ', metrics.accuracy_score(y_train_up, kb15_pred_train))\n",
    "\n",
    "# checking accuracy\n",
    "print('Test F1 score: ', metrics.f1_score(y_test, kb15_pred_test))\n",
    "print('Train F1 score: ', metrics.f1_score(y_train_up, kb15_pred_train))\n",
    "\n",
    "# checking accuracy\n",
    "print('Test Recall score: ', metrics.recall_score(y_test, kb15_pred_test))\n",
    "print('Train Recall score: ', metrics.recall_score(y_train_up, kb15_pred_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results['5. lr_kb15'] = (round(metrics.accuracy_score(y_test, kb15_pred_test),4), \n",
    "                           round(metrics.f1_score(y_test, kb15_pred_test),4),\n",
    "                     round(metrics.recall_score(y_test, kb15_pred_test),4))\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Logistic Regression with K best 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kb20_lr = LogisticRegression(solver='liblinear')\n",
    "\n",
    "kb20_lr.fit(X_train_kb20, y_train_up)\n",
    "\n",
    "\n",
    "kb20_pred_test = kb20_lr.predict(X_test_kb20)\n",
    "kb20_pred_train = kb20_lr.predict(X_train_kb20)\n",
    "\n",
    "\n",
    "# checking accuracy\n",
    "print('Test Accuracy score: ', metrics.accuracy_score(y_test, kb20_pred_test))\n",
    "print('Train Accuracy score: ', metrics.accuracy_score(y_train_up, kb20_pred_train))\n",
    "\n",
    "# checking accuracy\n",
    "print('Test F1 score: ', metrics.f1_score(y_test, kb20_pred_test))\n",
    "print('Train F1 score: ', metrics.f1_score(y_train_up, kb20_pred_train))\n",
    "\n",
    "# checking accuracy\n",
    "print('Test Recall score: ', metrics.recall_score(y_test, kb20_pred_test))\n",
    "print('Train Recall score: ', metrics.recall_score(y_train_up, kb20_pred_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results['6.lr_kb20'] = (round(metrics.accuracy_score(y_test, kb20_pred_test),4), \n",
    "                           round(metrics.f1_score(y_test, kb20_pred_test),4),\n",
    "                     round(metrics.recall_score(y_test, kb20_pred_test),4))\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Logistic Regression with K best 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kb25_lr = LogisticRegression(solver='liblinear')\n",
    "\n",
    "kb25_lr.fit(X_train_kb25, y_train_up)\n",
    "\n",
    "\n",
    "kb25_pred_test = kb25_lr.predict(X_test_kb25)\n",
    "kb25_pred_train = kb25_lr.predict(X_train_kb25)\n",
    "\n",
    "\n",
    "# checking accuracy\n",
    "print('Test Accuracy score: ', metrics.accuracy_score(y_test, kb25_pred_test))\n",
    "print('Train Accuracy score: ', metrics.accuracy_score(y_train_up, kb25_pred_train))\n",
    "\n",
    "# checking accuracy\n",
    "print('Test F1 score: ', metrics.f1_score(y_test, kb25_pred_test))\n",
    "print('Train F1 score: ', metrics.f1_score(y_train_up, kb25_pred_train))\n",
    "\n",
    "# checking accuracy\n",
    "print('Test Recall score: ', metrics.recall_score(y_test, kb25_pred_test))\n",
    "print('Train Recall score: ', metrics.recall_score(y_train_up, kb25_pred_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results['7. lr_kb25'] = (round(metrics.accuracy_score(y_test, kb25_pred_test),4), \n",
    "                           round(metrics.f1_score(y_test, kb25_pred_test),4),\n",
    "                     round(metrics.recall_score(y_test, kb25_pred_test),4))\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Logistic Regression with K best 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kb30_lr = LogisticRegression(solver='liblinear')\n",
    "\n",
    "kb30_lr.fit(X_train_kb30, y_train_up)\n",
    "\n",
    "\n",
    "kb30_pred_test = kb30_lr.predict(X_test_kb30)\n",
    "kb30_pred_train = kb30_lr.predict(X_train_kb30)\n",
    "\n",
    "\n",
    "# checking accuracy\n",
    "print('Test Accuracy score: ', metrics.accuracy_score(y_test, kb30_pred_test))\n",
    "print('Train Accuracy score: ', metrics.accuracy_score(y_train_up, kb30_pred_train))\n",
    "\n",
    "# checking accuracy\n",
    "print('Test F1 score: ', metrics.f1_score(y_test, kb30_pred_test))\n",
    "print('Train F1 score: ', metrics.f1_score(y_train_up, kb30_pred_train))\n",
    "\n",
    "# checking accuracy\n",
    "print('Test Recall score: ', metrics.recall_score(y_test, kb30_pred_test))\n",
    "print('Train Recal score: ', metrics.recall_score(y_train_up, kb30_pred_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results['8. lr_kb30'] = (round(metrics.accuracy_score(y_test, kb30_pred_test),4), \n",
    "                           round(metrics.f1_score(y_test, kb30_pred_test),4),\n",
    "                     round(metrics.recall_score(y_test, kb30_pred_test),4))\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Logistic Regression with K best 35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kb35_lr = LogisticRegression(solver='liblinear')\n",
    "\n",
    "kb35_lr.fit(X_train_kb35, y_train_up)\n",
    "\n",
    "\n",
    "kb35_pred_test = kb35_lr.predict(X_test_kb35)\n",
    "kb35_pred_train = kb35_lr.predict(X_train_kb35)\n",
    "\n",
    "\n",
    "# checking accuracy\n",
    "print('Test Accuracy score: ', metrics.accuracy_score(y_test, kb35_pred_test))\n",
    "print('Train Accuracy score: ', metrics.accuracy_score(y_train_up, kb35_pred_train))\n",
    "\n",
    "# checking accuracy\n",
    "print('Test F1 score: ', metrics.f1_score(y_test, kb35_pred_test))\n",
    "print('Train F1 score: ', metrics.f1_score(y_train_up, kb35_pred_train))\n",
    "\n",
    "# checking accuracy\n",
    "print('Test Recall score: ', metrics.recall_score(y_test, kb35_pred_test))\n",
    "print('Train Recall score: ', metrics.recall_score(y_train_up, kb35_pred_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results['9. lr_kb35'] = (round(metrics.accuracy_score(y_test, kb35_pred_test),4), \n",
    "                           round(metrics.f1_score(y_test, kb35_pred_test),4),\n",
    "                     round(metrics.recall_score(y_test, kb35_pred_test),4))\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.  K best for the Upsampled and  Scaled Train Set -KNN1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimal K for KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fumction to find the position of the maximum value in a list\n",
    "def max_value(l):\n",
    "    max_val = max(l)\n",
    "    max_idx = l.index(max_val)\n",
    "    return max_idx, max_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a container to track the scores\n",
    "k_scores1=[]\n",
    "\n",
    "#set up a loop to fit the model using a different values of K\n",
    "\n",
    "k_range = list(range(10, 35))\n",
    "for k in k_range:\n",
    "    knn1 = KNeighborsClassifier(n_neighbors=k)\n",
    "    #fit the model and get the score on a evaluation metric\n",
    "    knn1.fit(X_train_scaleu, y_train_up)\n",
    "    y_pred = knn1.predict(X_test_scale)\n",
    "    \n",
    "    acc2 = metrics.accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    k_scores1.append(acc2)\n",
    "\n",
    "#use the max_value function to find the K value that gives you the best accuracy pred \n",
    "idx, val = max_value(k_scores1)\n",
    "    \n",
    "print(idx+1, val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=11)\n",
    "\n",
    "knn.fit(X_train_scaleu, y_train_up)\n",
    "\n",
    "y_pred_test =knn.predict(X_test_scale)\n",
    "y_pred_train = knn.predict(X_train_scaleu)\n",
    "\n",
    "# checking accuracy\n",
    "print('Test Accuracy score: ', metrics.accuracy_score(y_test, y_pred_test))\n",
    "print('Train Accuracy score: ', metrics.accuracy_score(y_train_up, y_pred_train))\n",
    "\n",
    "# checking accuracy\n",
    "print('Test F1 score: ', metrics.f1_score(y_test, y_pred_test))\n",
    "print('Train F1 score: ', metrics.f1_score(y_train_up, y_pred_train))\n",
    "\n",
    "# checking accuracy\n",
    "print('Test Recall score: ', metrics.recall_score(y_test, y_pred_test))\n",
    "print('Train Recall score: ', metrics.recall_score(y_train_up, y_pred_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results['10. knn_upscaled'] = (round(metrics.accuracy_score(y_test, y_pred_test),4), \n",
    "                              round(metrics.f1_score(y_test, y_pred_test),4),\n",
    "                          round(metrics.recall_score(y_test, y_pred_test),4))\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. K best for the Imbalanced and  Scaled Train Set - KNN2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a container to track the scores\n",
    "k_scores2=[]\n",
    "\n",
    "#set up a loop to fit the model using a different values of K\n",
    "\n",
    "k_range = list(range(10, 35))\n",
    "for k in k_range:\n",
    "    knn2 = KNeighborsClassifier(n_neighbors=k)\n",
    "    #fit the model and get the score on a evaluation metric\n",
    "    knn2.fit(X_train_scaleI, y_train)\n",
    "    y_pred= knn2.predict(X_test_scale)\n",
    "    \n",
    "    acc2 = metrics.accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    k_scores2.append(acc2)\n",
    "\n",
    "#use the max_value function to find the K value that gives you the best accuracy pred \n",
    "idx, val = max_value(k_scores2)\n",
    "    \n",
    "print(idx+1, val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn2 = KNeighborsClassifier(n_neighbors=1)\n",
    "\n",
    "knn2.fit(X_train_scaleI, y_train)\n",
    "\n",
    "y_pred_test =knn2.predict(X_test_scale)\n",
    "y_pred_train = knn2.predict(X_train_scaleI)\n",
    "\n",
    "# checking accuracy\n",
    "print('Test Accuracy score: ', metrics.accuracy_score(y_test, y_pred_test))\n",
    "print('Train Accuracy score: ', metrics.accuracy_score(y_train, y_pred_train))\n",
    "\n",
    "# checking accuracy\n",
    "print('Test F1 score: ', metrics.f1_score(y_test, y_pred_test))\n",
    "print('Train F1 score: ', metrics.f1_score(y_train, y_pred_train))\n",
    "\n",
    "# checking accuracy\n",
    "print('Test Recall score: ', metrics.recall_score(y_test, y_pred_test))\n",
    "print('Train Recall; score: ', metrics.recall_score(y_train, y_pred_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results['11. knn2_imbscaled'] = (round(metrics.accuracy_score(y_test, y_pred_test),4), \n",
    "                             round(metrics.recall_score(y_test, y_pred_test),4),\n",
    "                            round(metrics.recall_score(y_test, y_pred_test),4))\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12. KNN for the Kbest25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scaling upsampled data\n",
    "scaler3 = MinMaxScaler()  \n",
    "scaler3.fit(X_train_kb25)\n",
    "\n",
    "X_train_kb25scale = scaler3.transform(X_train_kb25) \n",
    "\n",
    "knn3 = KNeighborsClassifier(n_neighbors=11)\n",
    "\n",
    "knn3.fit(X_train_kb25scale, y_train_up)\n",
    "\n",
    "y_pred_test =knn3.predict(X_test_kb25)\n",
    "y_pred_train = knn3.predict(X_train_kb25scale)\n",
    "\n",
    "# checking accuracy\n",
    "print('Test Accuracy score: ', metrics.accuracy_score(y_test, y_pred_test))\n",
    "print('Train Accuracy score: ', metrics.accuracy_score(y_train_up, y_pred_train))\n",
    "\n",
    "# checking accuracy\n",
    "print('Test F1 score: ', metrics.f1_score(y_test, y_pred_test))\n",
    "print('Train F1 score: ', metrics.f1_score(y_train_up, y_pred_train))\n",
    "\n",
    "# checking accuracy\n",
    "print('Test Recall score: ', metrics.recall_score(y_test, y_pred_test))\n",
    "print('Train Recall score: ', metrics.recall_score(y_train_up, y_pred_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results['12. knn3_kb25'] = (round(metrics.accuracy_score(y_test, y_pred_test),4), \n",
    "                             round(metrics.f1_score(y_test, y_pred_test),4),\n",
    "                            round(metrics.recall_score(y_test, y_pred_test),4))\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred_test)\n",
    "\n",
    "cm_display = ConfusionMatrixDisplay(cm).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13. KNN for Kbest 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scaling upsampled data\n",
    "scaler4 = MinMaxScaler()  \n",
    "scaler4.fit(X_train_kb30)\n",
    "\n",
    "X_train_kb30scale = scaler4.transform(X_train_kb30)\n",
    "                                     \n",
    "knn4 = KNeighborsClassifier(n_neighbors=11)\n",
    "\n",
    "knn4.fit(X_train_kb30scale, y_train_up)\n",
    "\n",
    "y_pred_test =knn4.predict(X_test_kb30)\n",
    "y_pred_train = knn4.predict(X_train_kb30scale)\n",
    "\n",
    "# checking accuracy\n",
    "print('Test Accuracy score: ', metrics.accuracy_score(y_test, y_pred_test))\n",
    "print('Train Accuracy score: ', metrics.accuracy_score(y_train_up, y_pred_train))\n",
    "\n",
    "# checking accuracy\n",
    "print('Test F1 score: ', metrics.f1_score(y_test, y_pred_test))\n",
    "print('Train F1 score: ', metrics.f1_score(y_train_up, y_pred_train))\n",
    "\n",
    "# checking accuracy\n",
    "print('Test Recall score: ', metrics.recall_score(y_test, y_pred_test))\n",
    "print('Train Recall score: ', metrics.recall_score(y_train_up, y_pred_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results['13. knn4_kb30'] = (round(metrics.accuracy_score(y_test, y_pred_test),4), \n",
    "                             round(metrics.recall_score(y_test, y_pred_test),4),\n",
    "                            round(metrics.recall_score(y_test, y_pred_test),4))\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14.Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtc1 = DecisionTreeClassifier(max_depth=10)\n",
    "\n",
    "#Train Decision Tree Classifier\n",
    "dtc1.fit(X_train_up,y_train_up)\n",
    "\n",
    "#predict the training set\n",
    "y_pred_train = dtc1.predict(X_train_up)\n",
    "\n",
    "#Predict the response for test dataset\n",
    "y_pred_test = dtc1.predict(X_test1)\n",
    "\n",
    "# Model Accuracy, how often is the classifier correct?\n",
    "# checking accuracy\n",
    "print('Test Accuracy score: ', metrics.accuracy_score(y_test, y_pred_test))\n",
    "print('Train Accuracy score: ', metrics.accuracy_score(y_train_up, y_pred_train))\n",
    "\n",
    "# checking accuracy\n",
    "print('Test F1 score: ', metrics.f1_score(y_test, y_pred_test))\n",
    "print('Train F1 score: ', metrics.f1_score(y_train_up, y_pred_train))\n",
    "\n",
    "# checking accuracy\n",
    "print('Test Recall score: ', metrics.recall_score(y_test, y_pred_test))\n",
    "print('Train Recall score: ', metrics.recall_score(y_train_up, y_pred_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results['14. dtc1_imbalanced'] = (round(metrics.accuracy_score(y_test, y_pred_test),4), \n",
    "                             round(metrics.f1_score(y_test, y_pred_test),4), \n",
    "                             round(metrics.recall_score(y_test, y_pred_test),4))\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred_test)\n",
    "\n",
    "cm_display = ConfusionMatrixDisplay(cm).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "kb10_lr = LogisticRegression(solver='liblinear')\n",
    "\n",
    "\n",
    "kb10_lr.fit(X_train_kb10, y_train_up)\n",
    "\n",
    "\n",
    "kb10_pred_test = kb10_lr.predict(X_test_kb10)\n",
    "kb10_pred_train = kb10_lr.predict(X_train_kb10)\n",
    "\n",
    "\n",
    "# checking accuracy\n",
    "print('Test Accuracy score: ', metrics.accuracy_score(y_test, kb10_pred_test))\n",
    "print('Train Accuracy score: ', metrics.accuracy_score(y_train_up, kb10_pred_train))\n",
    "\n",
    "# checking accuracy\n",
    "print('Test F1 score: ', metrics.f1_score(y_test, kb10_pred_test))\n",
    "print('Train F1 score: ', metrics.f1_score(y_train_up, kb10_pred_train))\n",
    "\n",
    "# checking accuracy\n",
    "print('Test Recall score: ', metrics.recall_score(y_test, kb10_pred_test))\n",
    "print('Train Recall score: ', metrics.recall_score(y_train_up, kb10_pred_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "features = list(X_train_kb10.columns)\n",
    "coeffs = (kb10_lr.coef_)[0].tolist()\n",
    "feat_coeff=list(zip(features, coeffs))\n",
    "feat_coeff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation of coefficients**\n",
    "\n",
    "1. **sex:** -1.62 is interpreted as the expected decrease in log odds for a female student being a heavy drinker. The odds ratio here is 1, which suggests that the chances of a female student being a heavy drinker is less likely to occur\n",
    "2. **absences:** 0.079 is interpreted as the expected change in log odds for a one- unit increase in absences\n",
    "3. **G1:** -0.19 is interpreted as the expected decrease in log odds of a heavy drinker\n",
    "4. **G2:** 0.027 is interpreted as the expected increease in log odds of a heavy drinker\n",
    "5. **G3:** -0.01 is interpreted as the expected decrease in log odds of heavy drinker\n",
    "6. **idle:** 0.028 is interpreted as the expected increease in log odds of a student being a heavy drinker\n",
    "7. **grade_avg:** -0.039 is interpreted as the expected decrease in log odds of a student being a heavy drinker\n",
    "8. **goout_2:** -0.65 is interpreted as the expected decrease in log odds of a student being a heavy drinker\n",
    "9. **goout_3:** -0.57 is interpreted as the expected decrease in log odds of a student being a heavy drinker\n",
    "10. **goout_5:** 1.05 is interpreted as the expected increase in log odds of a student being a heavy drinker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (10,10))\n",
    "sns.set_style('darkgrid')\n",
    "plt.title('Coefficients of Model Features')\n",
    "plt.xlabel('Coefficients')\n",
    "plt.xlim(-2, 1.5)\n",
    "plt.barh(y= features, width = coeffs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import confusion_matrix\n",
    "# from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "# cm = confusion_matrix(y_test, kb10_pred_test)\n",
    "\n",
    "# cm_display = ConfusionMatrixDisplay(cm).plot()\n",
    "# plt.savefig('images/Confusion_matrix.png')\n",
    "metrics.plot_confusion_matrix(kb10_lr, X_train_kb10, y_train_up)\n",
    "metrics.plot_confusion_matrix(kb10_lr, X_test_kb10, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Confusion Matrix Observation:**\n",
    "\n",
    "* True Positives : 19\n",
    "* True Negatives : 90\n",
    "* False Positives :14\n",
    "* False Negatives : 9\n",
    "\n",
    "The Recall Score of 0.6538 for this model tells us that the model predict an actual positive correctly 65% of the time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features of the model with the next best Recall score\n",
    "X_train_kb25.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
